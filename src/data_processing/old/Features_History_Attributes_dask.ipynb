{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d8ebf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b865fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = r\"K:\\CentraalDatamanagement\\PDC\\01_WIP\\01_Algemeen\\X_000002_DatakwaliteitBaseline\\DAMO_H_parquet\\DAMO_W.ws_Plattenormprofiel_H.parquet\"\n",
    "date_col = 'GDB_FROM_DATE'\n",
    "id_col = 'OBJECTID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c812835",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "853be68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Optional, Sequence, Union\n",
    "\n",
    "def last_attribute_change_dates(\n",
    "    path: Union[str, Sequence[str]],\n",
    "    object_id_col: str = \"OBJECTID\",\n",
    "    from_col: str = \"GDB_FROM_DATE\",\n",
    "    to_col: Optional[str] = \"GDB_TO_DATE\",\n",
    "    attributes: Optional[List[str]] = None,   # expliciet opgeven welke attributen; anders auto\n",
    "    exclude: Optional[List[str]] = None,       # kolommen om te negeren\n",
    "    treat_empty_as_nan: bool = True,           # '' behandelen als NaN bij vergelijking\n",
    "    npartitions: Optional[int] = None,         # optioneel herpartitioneren\n",
    "    engine: str = \"pyarrow\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Retourneert een Dask DataFrame met:\n",
    "      - 1 rij per OBJECTID\n",
    "      - Voor elke attribuutkolom: de laatste datum (GDB_FROM_DATE) waarop dit attribuut veranderde.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Lees alleen de metadata om kolomnamen te bepalen\n",
    "    ddf_meta = dd.read_parquet(path, engine=engine)\n",
    "\n",
    "    # 2) Bepaal attribuutkolommen\n",
    "    drop = {object_id_col, from_col}\n",
    "    if to_col:\n",
    "        drop.add(to_col)\n",
    "    if exclude:\n",
    "        drop |= set(exclude)\n",
    "    # verwijder gebruikelijke geometriekolommen indien aanwezig\n",
    "    for gcol in (\"geometry\", \"wkb_geometry\", \"geom\", \"GEOMETRY\"):\n",
    "        if gcol in ddf_meta.columns:\n",
    "            drop.add(gcol)\n",
    "\n",
    "    if attributes is None:\n",
    "        attr_cols = [c for c in ddf_meta.columns if c not in drop]\n",
    "    else:\n",
    "        missing = set(attributes) - set(ddf_meta.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Attribuutkolommen niet gevonden: {missing}\")\n",
    "        attr_cols = list(attributes)\n",
    "\n",
    "    # 3) Lees alleen benodigde kolommen\n",
    "    cols_needed = [object_id_col, from_col] + attr_cols\n",
    "    ddf = dd.read_parquet(path, engine=engine, columns=cols_needed)\n",
    "\n",
    "    # 4) Zorg dat from_col datetime is\n",
    "    ddf[from_col] = dd.to_datetime(ddf[from_col], errors=\"coerce\")\n",
    "\n",
    "    if npartitions:\n",
    "        ddf = ddf.repartition(npartitions=npartitions)\n",
    "\n",
    "    # 5) Per OBJECTID-groep de laatste wijzigingsdatums bepalen\n",
    "    def _per_object_last_change(pdf: pd.DataFrame,\n",
    "                                from_col: str,\n",
    "                                attr_cols: List[str],\n",
    "                                treat_empty_as_nan: bool):\n",
    "        # sorteer op tijd (stabiel)\n",
    "        g = pdf.sort_values(from_col, kind=\"mergesort\").reset_index(drop=True)\n",
    "        out = {}\n",
    "\n",
    "        # Voor elke attribuutkolom bepaal wanneer de waarde wijzigt t.o.v. vorige versie\n",
    "        for col in attr_cols:\n",
    "            s = g[col]\n",
    "\n",
    "            # optioneel lege strings als NaN behandelen (handig voor string-attributen)\n",
    "            if treat_empty_as_nan and s.dtype == \"object\":\n",
    "                s = s.replace(\"\", np.nan)\n",
    "\n",
    "            prev = s.shift(1)\n",
    "            # \"zelfde waarde\" = gelijk of beide NaN\n",
    "            same = (s == prev) | (s.isna() & prev.isna())\n",
    "            changed = ~same\n",
    "\n",
    "            if len(changed) > 0:\n",
    "                # De allereerste rij geldt als \"initiële wijziging\"\n",
    "                changed.iloc[0] = True\n",
    "\n",
    "            if changed.any():\n",
    "                last_date = g.loc[changed, from_col].max()\n",
    "            else:\n",
    "                # Geen rijen? (zou niet voorkomen) -> NaT\n",
    "                last_date = pd.NaT\n",
    "\n",
    "            out[col] = last_date\n",
    "\n",
    "        # 1 rij terug met alleen attribuutkolommen (OBJECTID komt straks uit de groupby-index)\n",
    "        return pd.DataFrame([out])\n",
    "\n",
    "    # 6) Meta voor Dask plannen: één lege DataFrame met attribuutkolommen (datetime64[ns])\n",
    "    meta = pd.DataFrame({col: pd.Series(dtype=\"datetime64[ns]\") for col in attr_cols})\n",
    "\n",
    "    # 7) Apply per OBJECTID; Dask shufflet groepen zodat elke groep compleet is\n",
    "    res = ddf.groupby(object_id_col).apply(\n",
    "        _per_object_last_change,\n",
    "        from_col=from_col,\n",
    "        attr_cols=attr_cols,\n",
    "        treat_empty_as_nan=treat_empty_as_nan,\n",
    "        meta=meta,\n",
    "        # split_out=...,  # optioneel (Dask >= 2023) om de output over meer partitions te spreiden\n",
    "    )\n",
    "\n",
    "    # groupby-index (OBJECTID) terug naar kolom\n",
    "    res = res.reset_index()\n",
    "\n",
    "    # => res: kolommen: OBJECTID + alle attribuutkolommen (als datums)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6076601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ddf_out = last_attribute_change_dates(\n",
    "    parquet_file,\n",
    "    object_id_col=\"OBJECTID\",\n",
    "    from_col=\"GDB_FROM_DATE\",\n",
    "    to_col=\"GDB_TO_DATE\",\n",
    "    # attributes=[\"NAAM\", \"TYPE\", \"STATUS\"],  # optioneel: selectie; anders automatisch\n",
    "    exclude=[\"GLOBALID\", \"CREATED_USER\", \"CREATED_DATE\", \"LAST_EDITED_USER\", \"LAST_EDITED_DATE\"],  # optioneel\n",
    "    npartitions=None,      # optioneel tunen\n",
    "    treat_empty_as_nan=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77eaf6ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mddf_out\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mK:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mCentraalDatamanagement\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mPDC\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m01_WIP\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m01_Algemeen\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mX_000002_DatakwaliteitBaseline\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDAMO_H_parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MBA\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:3325\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, **kwargs)\u001b[39m\n\u001b[32m   3322\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_parquet\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, **kwargs):\n\u001b[32m   3323\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdask_expr\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MBA\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\io\\parquet.py:661\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, compression, write_index, append, overwrite, ignore_divisions, partition_on, storage_options, custom_metadata, write_metadata_file, compute, compute_kwargs, schema, name_function, filesystem, engine, **kwargs)\u001b[39m\n\u001b[32m    637\u001b[39m         out = new_collection(\n\u001b[32m    638\u001b[39m             ToParquet(\n\u001b[32m    639\u001b[39m                 df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    657\u001b[39m             )\n\u001b[32m    658\u001b[39m         )\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m     out = \u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;66;03m# Invalidate the filesystem listing cache for the output path after write.\u001b[39;00m\n\u001b[32m    664\u001b[39m \u001b[38;5;66;03m# We do this before returning, even if `compute=False`. This helps ensure\u001b[39;00m\n\u001b[32m    665\u001b[39m \u001b[38;5;66;03m# that reading files that were just written succeeds.\u001b[39;00m\n\u001b[32m    666\u001b[39m fs.invalidate_cache(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MBA\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\dask\\base.py:378\u001b[39m, in \u001b[36mDaskMethodsMixin.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    355\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[32m    356\u001b[39m \n\u001b[32m    357\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    376\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     (result,) = \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MBA\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\dask\\base.py:686\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    683\u001b[39m     expr = expr.optimize()\n\u001b[32m    684\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m686\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MBA\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\queue.py:210\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ShutDown\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MBA\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\threading.py:373\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    375\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "ddf_out.to_parquet(r\"K:\\CentraalDatamanagement\\PDC\\01_WIP\\01_Algemeen\\X_000002_DatakwaliteitBaseline\\DAMO_H_parquet\", engine=\"pyarrow\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e773da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
